{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'sparse_tensor_dense_tensordot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0b35528ffa19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor_dense_tensordot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'sparse_tensor_dense_tensordot'"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import sparse_tensor\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import check_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import gen_sparse_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "# go/tf-wildcard-import\n",
    "# pylint: disable=wildcard-import\n",
    "from tensorflow.python.ops.gen_sparse_ops import *\n",
    "# pylint: enable=wildcard-import\n",
    "from tensorflow.python.util import compat, deprecation\n",
    "\n",
    "\n",
    "from tensorflow.python.ops.sparse_ops import sparse_tensor_dense_tensordot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "shape=[2,4,3]\n",
    "b  = tf.placeholder(\"float\", shape=shape)\n",
    "sp_a = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=[3,3])\n",
    "name = None\n",
    "axes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparse_tensor_dense_tensordot(sp_a, b, axes, name=None):\n",
    "    \n",
    "    def _tensordot_axes(a, axes):\n",
    "        \"\"\"Generates two sets of contraction axes for the two tensor arguments.\"\"\"\n",
    "        a_shape = a.get_shape()\n",
    "        if isinstance(axes, compat.integral_types):\n",
    "            if axes < 1:\n",
    "                raise ValueError(\"'axes' must be at least 1.\")\n",
    "            if a_shape.ndims is not None:\n",
    "                return math_ops.range(a_shape.ndims - axes, a_shape.ndims), math_ops.range(axes)\n",
    "            else:\n",
    "                rank = array_ops.rank(a)\n",
    "                return (math_ops.range(rank - axes, rank, dtype=dtypes.int32),\n",
    "                                math_ops.range(axes, dtype=dtypes.int32))\n",
    "        elif isinstance(axes, (list, tuple)):\n",
    "            if len(axes) != 2:\n",
    "                raise ValueError(\"'axes' must be an integer or have length 2.\")\n",
    "            a_axes = axes[0]\n",
    "            b_axes = axes[1]\n",
    "            if isinstance(a_axes, compat.integral_types) and \\\n",
    "                    isinstance(b_axes, compat.integral_types):\n",
    "                a_axes = [a_axes]\n",
    "                b_axes = [b_axes]\n",
    "            if len(a_axes) != len(b_axes):\n",
    "                raise ValueError(\n",
    "                        \"Different number of contraction axes 'a' and 'b', %s != %s.\" %\n",
    "                        (len(a_axes), len(b_axes)))\n",
    "            return a_axes, b_axes\n",
    "        else:\n",
    "            axes = ops.convert_to_tensor(axes, name=\"axes\", dtype=dtypes.int32)\n",
    "            return axes[0], axes[1]\n",
    "    \n",
    "    def _tensordot_reshape(a, axes, flipped=False):\n",
    "        \"\"\"Helper method to perform transpose and reshape for contraction op.\n",
    "\n",
    "        This method is helpful in reducing `math_ops.tensordot` to `math_ops.matmul`\n",
    "        using `array_ops.transpose` and `array_ops.reshape`. The method takes a\n",
    "        tensor and performs the correct transpose and reshape operation for a given\n",
    "        set of indices. It returns the reshaped tensor as well as a list of indices\n",
    "        necessary to reshape the tensor again after matrix multiplication.\n",
    "\n",
    "        Args:\n",
    "            a: `Tensor`.\n",
    "            axes: List or `int32` `Tensor` of unique indices specifying valid axes of\n",
    "             `a`.\n",
    "            flipped: An optional `bool`. Defaults to `False`. If `True`, the method\n",
    "                assumes that `a` is the second argument in the contraction operation.\n",
    "\n",
    "        Returns:\n",
    "            A tuple `(reshaped_a, free_dims, free_dims_static)` where `reshaped_a` is\n",
    "            the tensor `a` reshaped to allow contraction via `matmul`, `free_dims` is\n",
    "            either a list of integers or an `int32` `Tensor`, depending on whether\n",
    "            the shape of a is fully specified, and free_dims_static is either a list\n",
    "            of integers and None values, or None, representing the inferred\n",
    "            static shape of the free dimensions\n",
    "        \"\"\"\n",
    "        if a.get_shape().is_fully_defined() and isinstance(axes, (list, tuple)):\n",
    "            shape_a = a.get_shape().as_list()\n",
    "            axes = [i if i >= 0 else i + len(shape_a) for i in axes]\n",
    "            free = [i for i in xrange(len(shape_a)) if i not in axes]\n",
    "            free_dims = [shape_a[i] for i in free]\n",
    "            prod_free = int(np.prod([shape_a[i] for i in free]))\n",
    "            prod_axes = int(np.prod([shape_a[i] for i in axes]))\n",
    "            perm = list(axes) + free if flipped else free + list(axes)\n",
    "            new_shape = [prod_axes, prod_free] if flipped else [prod_free, prod_axes]\n",
    "            reshaped_a = array_ops.reshape(array_ops.transpose(a, perm), new_shape)\n",
    "            return reshaped_a, free_dims, free_dims\n",
    "        else:\n",
    "            if a.get_shape().ndims is not None and isinstance(axes, (list, tuple)):\n",
    "                shape_a = a.get_shape().as_list()\n",
    "                axes = [i if i >= 0 else i + len(shape_a) for i in axes]\n",
    "                free = [i for i in xrange(len(shape_a)) if i not in axes]\n",
    "                free_dims_static = [shape_a[i] for i in free]\n",
    "            else:\n",
    "                free_dims_static = None\n",
    "            shape_a = array_ops.shape(a)\n",
    "            rank_a = array_ops.rank(a)\n",
    "            axes = ops.convert_to_tensor(axes, dtype=dtypes.int32, name=\"axes\")\n",
    "            axes = math_ops.cast(axes >= 0, dtypes.int32) * axes + math_ops.cast(\n",
    "                    axes < 0, dtypes.int32) * (\n",
    "                            axes + rank_a)\n",
    "            free, _ = array_ops.setdiff1d(math_ops.range(rank_a), axes)\n",
    "            free_dims = array_ops.gather(shape_a, free)\n",
    "            axes_dims = array_ops.gather(shape_a, axes)\n",
    "            prod_free_dims = math_ops.reduce_prod(free_dims)\n",
    "            prod_axes_dims = math_ops.reduce_prod(axes_dims)\n",
    "            perm = array_ops.concat([axes_dims, free_dims], 0)\n",
    "            if flipped:\n",
    "                perm = array_ops.concat([axes, free], 0)\n",
    "                new_shape = array_ops.stack([prod_axes_dims, prod_free_dims])\n",
    "            else:\n",
    "                perm = array_ops.concat([free, axes], 0)\n",
    "                new_shape = array_ops.stack([prod_free_dims, prod_axes_dims])\n",
    "            reshaped_a = array_ops.reshape(array_ops.transpose(a, perm), new_shape)\n",
    "            return reshaped_a, free_dims, free_dims_static\n",
    "\n",
    "    def _tensordot_sparse_reshape(a, axes, flipped=False):\n",
    "        \"\"\"Helper method to perform transpose and reshape for contraction op.\n",
    "\n",
    "        This method is helpful in reducing `math_ops.tensordot` to `math_ops.matmul`\n",
    "        using `array_ops.transpose` and `array_ops.reshape`. The method takes a\n",
    "        tensor and performs the correct transpose and reshape operation for a given\n",
    "        set of indices. It returns the reshaped tensor as well as a list of indices\n",
    "        necessary to reshape the tensor again after matrix multiplication.\n",
    "\n",
    "        Args:\n",
    "            a: `Tensor`.\n",
    "            axes: List or `int32` `Tensor` of unique indices specifying valid axes of\n",
    "             `a`.\n",
    "            flipped: An optional `bool`. Defaults to `False`. If `True`, the method\n",
    "                assumes that `a` is the second argument in the contraction operation.\n",
    "\n",
    "        Returns:\n",
    "            A tuple `(reshaped_a, free_dims, free_dims_static)` where `reshaped_a` is\n",
    "            the tensor `a` reshaped to allow contraction via `matmul`, `free_dims` is\n",
    "            either a list of integers or an `int32` `Tensor`, depending on whether\n",
    "            the shape of a is fully specified, and free_dims_static is either a list\n",
    "            of integers and None values, or None, representing the inferred\n",
    "            static shape of the free dimensions\n",
    "        \"\"\"\n",
    "        if a.get_shape().is_fully_defined() and isinstance(axes, (list, tuple)):\n",
    "            shape_a = a.get_shape().as_list()\n",
    "            axes = [i if i >= 0 else i + len(shape_a) for i in axes]\n",
    "            free = [i for i in xrange(len(shape_a)) if i not in axes]\n",
    "            free_dims = [shape_a[i] for i in free]\n",
    "            prod_free = int(np.prod([shape_a[i] for i in free]))\n",
    "            prod_axes = int(np.prod([shape_a[i] for i in axes]))\n",
    "            perm = list(axes) + free if flipped else free + list(axes)\n",
    "            new_shape = [prod_axes, prod_free] if flipped else [prod_free, prod_axes]\n",
    "            reshaped_a = sparse_reshape(sparse_transpose(a, perm), new_shape)\n",
    "            return reshaped_a, free_dims, free_dims\n",
    "        else:\n",
    "            if a.get_shape().ndims is not None and isinstance(axes, (list, tuple)):\n",
    "                shape_a = a.get_shape().as_list()\n",
    "                axes = [i if i >= 0 else i + len(shape_a) for i in axes]\n",
    "                free = [i for i in xrange(len(shape_a)) if i not in axes]\n",
    "                free_dims_static = [shape_a[i] for i in free]\n",
    "            else:\n",
    "                free_dims_static = None\n",
    "            shape_a = array_ops.shape(a)\n",
    "            rank_a = array_ops.rank(a)\n",
    "            axes = ops.convert_to_tensor(axes, dtype=dtypes.int32, name=\"axes\")\n",
    "            axes = math_ops.cast(axes >= 0, dtypes.int32) * axes + math_ops.cast(\n",
    "                    axes < 0, dtypes.int32) * (\n",
    "                            axes + rank_a)\n",
    "            free, _ = array_ops.setdiff1d(math_ops.range(rank_a), axes)\n",
    "            free_dims = array_ops.gather(shape_a, free)\n",
    "            axes_dims = array_ops.gather(shape_a, axes)\n",
    "            prod_free_dims = math_ops.reduce_prod(free_dims)\n",
    "            prod_axes_dims = math_ops.reduce_prod(axes_dims)\n",
    "            perm = array_ops.concat([axes_dims, free_dims], 0)\n",
    "            if flipped:\n",
    "                perm = array_ops.concat([axes, free], 0)\n",
    "                new_shape = array_ops.stack([prod_axes_dims, prod_free_dims])\n",
    "            else:\n",
    "                perm = array_ops.concat([free, axes], 0)\n",
    "                new_shape = array_ops.stack([prod_free_dims, prod_axes_dims])\n",
    "            reshaped_a = sparse_reshape(sparse_transpose(a, perm), new_shape)\n",
    "            return reshaped_a, free_dims, free_dims_static\n",
    "\n",
    "    with ops.name_scope(name, \"Tensordot\", [sp_a, b, axes]) as name:\n",
    "        sp_a = sparse_tensor.convert_to_tensor_or_sparse_tensor(sp_a, name=\"sp_a\")\n",
    "        b = ops.convert_to_tensor(b, name=\"b\")\n",
    "        a_axes, b_axes = _tensordot_axes(sp_a, axes)\n",
    "        a_reshape, a_free_dims, a_free_dims_static = _tensordot_sparse_reshape(sp_a, a_axes)\n",
    "        b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n",
    "            b, b_axes, True)\n",
    "        ab_matmul = tf.sparse_tensor_dense_matmul(a_reshape, b_reshape)\n",
    "        if isinstance(a_free_dims, list) and isinstance(b_free_dims, list):\n",
    "            return array_ops.reshape(ab_matmul, a_free_dims + b_free_dims, name=name)\n",
    "        else:\n",
    "            a_free_dims = ops.convert_to_tensor(a_free_dims, dtype=dtypes.int32)\n",
    "            b_free_dims = ops.convert_to_tensor(b_free_dims, dtype=dtypes.int32)\n",
    "            product = array_ops.reshape(\n",
    "                    ab_matmul, array_ops.concat([a_free_dims, b_free_dims], 0), name=name)\n",
    "            if a_free_dims_static is not None and b_free_dims_static is not None:\n",
    "                product.set_shape(a_free_dims_static + b_free_dims_static)\n",
    "            return product\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Tensordot_42:0' shape=<unknown> dtype=int32>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_tensor_dense_tensordot(sp_a,b,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _random_subset(m, n):\n",
    "    assert m <= n\n",
    "    return (np.random.permutation(n)[:m]).astype(np.int32)\n",
    "\n",
    "def _generate_random_tensors_and_dims():\n",
    "    a_shape = np.random.random_integers(1, _MAXDIM, rank_a_)\n",
    "    b_shape = np.random.random_integers(1, _MAXDIM, rank_b_)\n",
    "    shared_shape = np.random.random_integers(1, _MAXDIM, num_dims_)\n",
    "    a_dims = _random_subset(num_dims_, rank_a_)\n",
    "    b_dims = _random_subset(num_dims_, rank_b_)\n",
    "    for i in range(num_dims_):\n",
    "        a_shape[a_dims[i]] = shared_shape[i]\n",
    "        b_shape[b_dims[i]] = shared_shape[i]\n",
    "    a = np.random.uniform(\n",
    "            low=-1.0, high=1.0,\n",
    "            size=np.prod(a_shape)).reshape(a_shape).astype(dtype_)\n",
    "    b = np.random.uniform(\n",
    "            low=-1.0, high=1.0,\n",
    "            size=np.prod(b_shape)).reshape(b_shape).astype(dtype_)\n",
    "    return a, b, a_dims, b_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _generate_random_tensors_and_dims():\n",
    "    a_shape = np.random.random_integers(1, _MAXDIM, rank_a_)\n",
    "    b_shape = np.random.random_integers(1, _MAXDIM, rank_b_)\n",
    "    shared_shape = np.random.random_integers(1, _MAXDIM, num_dims_)\n",
    "    a_dims = _random_subset(num_dims_, rank_a_)\n",
    "    b_dims = _random_subset(num_dims_, rank_b_)\n",
    "    for i in range(num_dims_):\n",
    "        a_shape[a_dims[i]] = shared_shape[i]\n",
    "        b_shape[b_dims[i]] = shared_shape[i]\n",
    "    a = np.random.uniform(\n",
    "            low=-1.0, high=1.0,\n",
    "            size=np.prod(a_shape)).reshape(a_shape).astype(dtype_)\n",
    "    b = np.random.uniform(\n",
    "            low=-1.0, high=1.0,\n",
    "            size=np.prod(b_shape)).reshape(b_shape).astype(dtype_)\n",
    "    \n",
    "    a_indices = np.dstack(tuple(np.nonzero(a)))[0]\n",
    "    a_shape = np.array(a.shape)\n",
    "    a_values = a[np.nonzero(a)]\n",
    "    \n",
    "    return a, b, a_dims, b_dims, (a_indices,a_values,a_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype_ = np.int32\n",
    "rank_a_ = 2\n",
    "rank_b_ = 4\n",
    "num_dims_ = 1 \n",
    "dynamic_shape = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = [5] * num_dims_\n",
    "a_np = np.random.uniform(\n",
    "        low=-1.0, high=1.0, size=np.prod(shape)).reshape(shape).astype(dtype_)\n",
    "b_np = np.random.uniform(\n",
    "        low=-1.0, high=1.0, size=np.prod(shape)).reshape(shape).astype(dtype_)\n",
    "all_axes = [1]\n",
    "if a_np.ndim > 1:\n",
    "    all_axes.append(a_np.ndim - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for axes in all_axes:\n",
    "    np_ans = np.tensordot(a_np, b_np, axes=axes)\n",
    "    with self.test_session(use_gpu=True) as sess:\n",
    "        if dynamic_shape_:\n",
    "            sp_a = array_ops.sparse_placeholder(dtype_)\n",
    "            b = array_ops.placeholder(dtype_)\n",
    "            c = sparse_ops.sparse_tensor_dense_tensordot(sp_a, b, axes=axes)\n",
    "            tf_ans = sess.run(c, feed_dict={sp_a: sp_a_np, b: b_np})\n",
    "        else:\n",
    "            \n",
    "            sp_a = sparse_tensor.SparseTensor(indices=sp_a_np[0],values=sp_a_np[1],dense_shape=sp_a_np[2])\n",
    "            tf_ans = sparse_ops.sparse_tensor_dense_tensordot(sp_a, b_np, axes=axes).eval()\n",
    "    self.assertAllClose(tf_ans, np_ans, rtol=tol, atol=tol)\n",
    "    self.assertAllEqual(tf_ans.shape, np_ans.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
